{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fH9v8cGdmdM3",
        "outputId": "5241e868-342d-4156-8f86-8c166789174f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting kaggle\n",
            "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▋                          | 10 kB 31.4 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 20 kB 38.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 30 kB 26.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 40 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 51 kB 16.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 58 kB 5.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=15ff768dd1cf0c0a0858ffae7795e1e3476fb14005228179f669572ac81f6089\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.12\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwLO37FiGtgB"
      },
      "outputs": [],
      "source": [
        "\n",
        "!mkdir convnext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwLEd0gdPbSc"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as ttf\n",
        "from torchvision import models\n",
        "import torchsummary\n",
        "from torchsummary import summary\n",
        "import os\n",
        "import os.path as osp\n",
        "\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scOnMklwWBY6"
      },
      "source": [
        "# Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BksgPdkQwwb"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('{\"username\":\" \",\"key\":\" \"}') # Put your kaggle username & key here\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oFjaJTaRjT7",
        "outputId": "6b12d113-e8eb-4afb-a11d-295b4fd10e64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading 11-785-s22-hw2p2-classification.zip to /content\n",
            " 99% 2.34G/2.35G [00:09<00:00, 192MB/s]\n",
            "100% 2.35G/2.35G [00:10<00:00, 253MB/s]\n",
            "Downloading 11-785-s22-hw2p2-verification.zip to /content\n",
            " 95% 249M/263M [00:03<00:00, 65.0MB/s]\n",
            "100% 263M/263M [00:03<00:00, 71.6MB/s]\n",
            "11-785-s22-hw2p2-classification.zip   sample_data\n",
            "11-785-s22-hw2p2-verification.zip     train_subset\n",
            "classification\t\t\t      verification\n",
            "classification_sample_submission.csv  verification_sample_submission.csv\n",
            "convnext\n"
          ]
        }
      ],
      "source": [
        "# # !kaggle competitions down!kaggle competitions download -c 11-785-s22-hw2p2-classification\n",
        "!kaggle competitions download -c 11-785-s22-hw2p2-classification\n",
        "!kaggle competitions download -c 11-785-s22-hw2p2-verification\n",
        "\n",
        "!unzip -q 11-785-s22-hw2p2-classification.zip\n",
        "!unzip -q 11-785-s22-hw2p2-verification.zip\n",
        "\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBTLCyocZBGS"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "\n",
        "\"\"\"\n",
        "The well-accepted SGD batch_size & lr combination for CNN classification is 256 batch size for 0.1 learning rate.\n",
        "When changing batch size for SGD, follow the linear scaling rule - halving batch size -> halve learning rate, etc.\n",
        "This is less theoretically supported for Adam, but in my experience, it's a decent ballpark estimate.\n",
        "\"\"\"\n",
        "batch_size = 256\n",
        "lr = 0.1\n",
        "epochs = 60"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIqmojPaWD0H"
      },
      "source": [
        "#  Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPmy-VSoy1KV"
      },
      "outputs": [],
      "source": [
        "class InvertedResidualBlock(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 ):\n",
        "        super().__init__() # Just have to do this for all nn.Module classes\n",
        "\n",
        "\n",
        "        #Depthwise Convolution\n",
        "        self.spatial_mixing = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=7, padding =3,\n",
        "                    stride = 1, groups = in_channels, bias=False),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "\n",
        "        )\n",
        "\n",
        "        # Expand Ratio is like 4, so hidden_dim >> in_channels\n",
        "        hidden_dim = in_channels * 4\n",
        "\n",
        "        #Pointwise Convolution\n",
        "        self.feature_mixing = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_dim, kernel_size=1, padding =0,\n",
        "                    stride = 1, bias=False),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "\n",
        "        self.bottleneck_channels = nn.Sequential(\n",
        "             nn.Conv2d(hidden_dim,out_channels,kernel_size=1,stride=1,padding=0,bias=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.spatial_mixing(x)\n",
        "        out = self.feature_mixing(out)\n",
        "        out = self.bottleneck_channels(out)\n",
        "        return x + out\n",
        "\n",
        "class ConvNext(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes= 7000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        \"\"\"\n",
        "        First couple of layers are special, just do them here.\n",
        "        This is called the \"stem\". Usually, methods use it to downsample or twice.\n",
        "        \"\"\"\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, kernel_size=4, stride=4),\n",
        "            nn.BatchNorm2d(96),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.stage_cfgs = [\n",
        "            # expand_ratio, channels, # blocks, stride of first block\n",
        "            [4,  96, 3, 1],\n",
        "            [4,  192, 3, 1],\n",
        "            [4,  384, 9, 1],\n",
        "            [4,  768, 3, 1],\n",
        "\n",
        "        ]\n",
        "\n",
        "        in_channels = 96\n",
        "        layers = []\n",
        "\n",
        "\n",
        "        #BLOCK TYPE 1 - 3 TIMES\n",
        "        for i in range(3):\n",
        "            layers.append(InvertedResidualBlock(\n",
        "                in_channels=96,\n",
        "                out_channels=96))\n",
        "\n",
        "        layers.append(nn.BatchNorm2d(96))\n",
        "        layers.append(nn.Conv2d(96,192,kernel_size=2,stride=2))\n",
        "\n",
        "        #BLOCK TYPE 2 - 3 TIMES\n",
        "        for i in range(3):\n",
        "            layers.append(InvertedResidualBlock(\n",
        "                in_channels=192,\n",
        "                out_channels=192))\n",
        "\n",
        "        layers.append(nn.BatchNorm2d(192))\n",
        "        layers.append(nn.Conv2d(192,384,kernel_size=2,stride=2))\n",
        "\n",
        "        #BLOCK TYPE 3 - 9 TIMES\n",
        "        for i in range(9):\n",
        "            layers.append(InvertedResidualBlock(\n",
        "                in_channels=384,\n",
        "                out_channels=384))\n",
        "\n",
        "        layers.append(nn.BatchNorm2d(384))\n",
        "        layers.append(nn.Conv2d(384,768,kernel_size=2,stride=2))\n",
        "\n",
        "        #BLOCK TYPE 4 - 3 TIMES\n",
        "        for i in range(3):\n",
        "            layers.append(InvertedResidualBlock(\n",
        "                in_channels=768,\n",
        "                out_channels=768))\n",
        "\n",
        "\n",
        "\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "        self.mid_cls_layer = nn.Sequential(\n",
        "\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "        self.final_cls_layer = nn.Sequential(nn.Linear(768,num_classes),)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x,return_feats=False):\n",
        "        out = self.stem(x)\n",
        "        out = self.layers(out)\n",
        "\n",
        "        feats = self.mid_cls_layer(out)\n",
        "\n",
        "        out = self.final_cls_layer(feats)\n",
        "\n",
        "\n",
        "        if return_feats:\n",
        "            return feats\n",
        "        else:\n",
        "            return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awE5BxlqX2o7"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Transforms (data augmentation) is quite important for this task.\n",
        "Go explore https://pytorch.org/vision/stable/transforms.html for more details\n",
        "\"\"\"\n",
        "DATA_DIR = \"/content\"\n",
        "TRAIN_DIR = osp.join(DATA_DIR, \"classification/classification/train\") \n",
        "VAL_DIR = osp.join(DATA_DIR, \"classification/classification/dev\")\n",
        "TEST_DIR = osp.join(DATA_DIR, \"classification/classification/test\")\n",
        "\n",
        "\n",
        "\n",
        "train_transforms = [ttf.RandAugment(),\n",
        "                    ttf.RandomHorizontalFlip(),\n",
        "                    ttf.ColorJitter((0.8,1.2),(0.8,1.2),(0.8,1.2)),\n",
        "                    ttf.ToTensor()]\n",
        "val_transforms = [ttf.ToTensor()]\n",
        "\n",
        "train_dataset = torchvision.datasets.ImageFolder(TRAIN_DIR,\n",
        "                                                 transform=ttf.Compose(train_transforms))\n",
        "val_dataset = torchvision.datasets.ImageFolder(VAL_DIR,\n",
        "                                               transform=ttf.Compose(val_transforms))\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                          shuffle=True, drop_last=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                        drop_last=True, num_workers=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZCn0qHuZRKj"
      },
      "source": [
        "# Setup everything for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UowI9OcUYPjP",
        "outputId": "99f2b66e-4655-49c9-bcff-fd6300fd5275"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Params: 33155224\n"
          ]
        }
      ],
      "source": [
        "model = ConvNext()\n",
        "# model.load_state_dict(torch.load('')) # Add modle location to resume training from a checkpoint\n",
        "model.cuda()\n",
        "\n",
        "\n",
        "num_trainable_parameters = 0\n",
        "for p in model.parameters():\n",
        "    num_trainable_parameters += p.numel()\n",
        "print(\"Number of Params: {}\".format(num_trainable_parameters))\n",
        "\n",
        "# TODO: What criterion do we use for this task?\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.25)\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4,nesterov=True)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(train_loader) * epochs))\n",
        "# T_max is \"how many times will i call scheduler.step() until it reaches 0 lr?\"\n",
        "\n",
        "#Using Mixed Precision\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkSrFbfKmb0b",
        "outputId": "b58e444a-a039-4887-fc00-aea1c6403c0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 96, 56, 56]           4,704\n",
            "       BatchNorm2d-2           [-1, 96, 56, 56]             192\n",
            "            Conv2d-3           [-1, 96, 56, 56]           4,704\n",
            "       BatchNorm2d-4           [-1, 96, 56, 56]             192\n",
            "            Conv2d-5          [-1, 384, 56, 56]          36,864\n",
            "              GELU-6          [-1, 384, 56, 56]               0\n",
            "            Conv2d-7           [-1, 96, 56, 56]          36,864\n",
            "InvertedResidualBlock-8           [-1, 96, 56, 56]               0\n",
            "            Conv2d-9           [-1, 96, 56, 56]           4,704\n",
            "      BatchNorm2d-10           [-1, 96, 56, 56]             192\n",
            "           Conv2d-11          [-1, 384, 56, 56]          36,864\n",
            "             GELU-12          [-1, 384, 56, 56]               0\n",
            "           Conv2d-13           [-1, 96, 56, 56]          36,864\n",
            "InvertedResidualBlock-14           [-1, 96, 56, 56]               0\n",
            "           Conv2d-15           [-1, 96, 56, 56]           4,704\n",
            "      BatchNorm2d-16           [-1, 96, 56, 56]             192\n",
            "           Conv2d-17          [-1, 384, 56, 56]          36,864\n",
            "             GELU-18          [-1, 384, 56, 56]               0\n",
            "           Conv2d-19           [-1, 96, 56, 56]          36,864\n",
            "InvertedResidualBlock-20           [-1, 96, 56, 56]               0\n",
            "      BatchNorm2d-21           [-1, 96, 56, 56]             192\n",
            "           Conv2d-22          [-1, 192, 28, 28]          73,920\n",
            "           Conv2d-23          [-1, 192, 28, 28]           9,408\n",
            "      BatchNorm2d-24          [-1, 192, 28, 28]             384\n",
            "           Conv2d-25          [-1, 768, 28, 28]         147,456\n",
            "             GELU-26          [-1, 768, 28, 28]               0\n",
            "           Conv2d-27          [-1, 192, 28, 28]         147,456\n",
            "InvertedResidualBlock-28          [-1, 192, 28, 28]               0\n",
            "           Conv2d-29          [-1, 192, 28, 28]           9,408\n",
            "      BatchNorm2d-30          [-1, 192, 28, 28]             384\n",
            "           Conv2d-31          [-1, 768, 28, 28]         147,456\n",
            "             GELU-32          [-1, 768, 28, 28]               0\n",
            "           Conv2d-33          [-1, 192, 28, 28]         147,456\n",
            "InvertedResidualBlock-34          [-1, 192, 28, 28]               0\n",
            "           Conv2d-35          [-1, 192, 28, 28]           9,408\n",
            "      BatchNorm2d-36          [-1, 192, 28, 28]             384\n",
            "           Conv2d-37          [-1, 768, 28, 28]         147,456\n",
            "             GELU-38          [-1, 768, 28, 28]               0\n",
            "           Conv2d-39          [-1, 192, 28, 28]         147,456\n",
            "InvertedResidualBlock-40          [-1, 192, 28, 28]               0\n",
            "      BatchNorm2d-41          [-1, 192, 28, 28]             384\n",
            "           Conv2d-42          [-1, 384, 14, 14]         295,296\n",
            "           Conv2d-43          [-1, 384, 14, 14]          18,816\n",
            "      BatchNorm2d-44          [-1, 384, 14, 14]             768\n",
            "           Conv2d-45         [-1, 1536, 14, 14]         589,824\n",
            "             GELU-46         [-1, 1536, 14, 14]               0\n",
            "           Conv2d-47          [-1, 384, 14, 14]         589,824\n",
            "InvertedResidualBlock-48          [-1, 384, 14, 14]               0\n",
            "           Conv2d-49          [-1, 384, 14, 14]          18,816\n",
            "      BatchNorm2d-50          [-1, 384, 14, 14]             768\n",
            "           Conv2d-51         [-1, 1536, 14, 14]         589,824\n",
            "             GELU-52         [-1, 1536, 14, 14]               0\n",
            "           Conv2d-53          [-1, 384, 14, 14]         589,824\n",
            "InvertedResidualBlock-54          [-1, 384, 14, 14]               0\n",
            "           Conv2d-55          [-1, 384, 14, 14]          18,816\n",
            "      BatchNorm2d-56          [-1, 384, 14, 14]             768\n",
            "           Conv2d-57         [-1, 1536, 14, 14]         589,824\n",
            "             GELU-58         [-1, 1536, 14, 14]               0\n",
            "           Conv2d-59          [-1, 384, 14, 14]         589,824\n",
            "InvertedResidualBlock-60          [-1, 384, 14, 14]               0\n",
            "           Conv2d-61          [-1, 384, 14, 14]          18,816\n",
            "      BatchNorm2d-62          [-1, 384, 14, 14]             768\n",
            "           Conv2d-63         [-1, 1536, 14, 14]         589,824\n",
            "             GELU-64         [-1, 1536, 14, 14]               0\n",
            "           Conv2d-65          [-1, 384, 14, 14]         589,824\n",
            "InvertedResidualBlock-66          [-1, 384, 14, 14]               0\n",
            "           Conv2d-67          [-1, 384, 14, 14]          18,816\n",
            "      BatchNorm2d-68          [-1, 384, 14, 14]             768\n",
            "           Conv2d-69         [-1, 1536, 14, 14]         589,824\n",
            "             GELU-70         [-1, 1536, 14, 14]               0\n",
            "           Conv2d-71          [-1, 384, 14, 14]         589,824\n",
            "InvertedResidualBlock-72          [-1, 384, 14, 14]               0\n",
            "           Conv2d-73          [-1, 384, 14, 14]          18,816\n",
            "      BatchNorm2d-74          [-1, 384, 14, 14]             768\n",
            "           Conv2d-75         [-1, 1536, 14, 14]         589,824\n",
            "             GELU-76         [-1, 1536, 14, 14]               0\n",
            "           Conv2d-77          [-1, 384, 14, 14]         589,824\n",
            "InvertedResidualBlock-78          [-1, 384, 14, 14]               0\n",
            "           Conv2d-79          [-1, 384, 14, 14]          18,816\n",
            "      BatchNorm2d-80          [-1, 384, 14, 14]             768\n",
            "           Conv2d-81         [-1, 1536, 14, 14]         589,824\n",
            "             GELU-82         [-1, 1536, 14, 14]               0\n",
            "           Conv2d-83          [-1, 384, 14, 14]         589,824\n",
            "InvertedResidualBlock-84          [-1, 384, 14, 14]               0\n",
            "           Conv2d-85          [-1, 384, 14, 14]          18,816\n",
            "      BatchNorm2d-86          [-1, 384, 14, 14]             768\n",
            "           Conv2d-87         [-1, 1536, 14, 14]         589,824\n",
            "             GELU-88         [-1, 1536, 14, 14]               0\n",
            "           Conv2d-89          [-1, 384, 14, 14]         589,824\n",
            "InvertedResidualBlock-90          [-1, 384, 14, 14]               0\n",
            "           Conv2d-91          [-1, 384, 14, 14]          18,816\n",
            "      BatchNorm2d-92          [-1, 384, 14, 14]             768\n",
            "           Conv2d-93         [-1, 1536, 14, 14]         589,824\n",
            "             GELU-94         [-1, 1536, 14, 14]               0\n",
            "           Conv2d-95          [-1, 384, 14, 14]         589,824\n",
            "InvertedResidualBlock-96          [-1, 384, 14, 14]               0\n",
            "      BatchNorm2d-97          [-1, 384, 14, 14]             768\n",
            "           Conv2d-98            [-1, 768, 7, 7]       1,180,416\n",
            "           Conv2d-99            [-1, 768, 7, 7]          37,632\n",
            "     BatchNorm2d-100            [-1, 768, 7, 7]           1,536\n",
            "          Conv2d-101           [-1, 3072, 7, 7]       2,359,296\n",
            "            GELU-102           [-1, 3072, 7, 7]               0\n",
            "          Conv2d-103            [-1, 768, 7, 7]       2,359,296\n",
            "InvertedResidualBlock-104            [-1, 768, 7, 7]               0\n",
            "          Conv2d-105            [-1, 768, 7, 7]          37,632\n",
            "     BatchNorm2d-106            [-1, 768, 7, 7]           1,536\n",
            "          Conv2d-107           [-1, 3072, 7, 7]       2,359,296\n",
            "            GELU-108           [-1, 3072, 7, 7]               0\n",
            "          Conv2d-109            [-1, 768, 7, 7]       2,359,296\n",
            "InvertedResidualBlock-110            [-1, 768, 7, 7]               0\n",
            "          Conv2d-111            [-1, 768, 7, 7]          37,632\n",
            "     BatchNorm2d-112            [-1, 768, 7, 7]           1,536\n",
            "          Conv2d-113           [-1, 3072, 7, 7]       2,359,296\n",
            "            GELU-114           [-1, 3072, 7, 7]               0\n",
            "          Conv2d-115            [-1, 768, 7, 7]       2,359,296\n",
            "InvertedResidualBlock-116            [-1, 768, 7, 7]               0\n",
            "AdaptiveAvgPool2d-117            [-1, 768, 1, 1]               0\n",
            "         Flatten-118                  [-1, 768]               0\n",
            "          Linear-119                 [-1, 7000]       5,383,000\n",
            "================================================================\n",
            "Total params: 33,155,224\n",
            "Trainable params: 33,155,224\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 207.07\n",
            "Params size (MB): 126.48\n",
            "Estimated Total Size (MB): 334.12\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "summary(model,(3,224,224))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzM11HtcboYv"
      },
      "source": [
        "# Let's train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFyqdbSFubwh"
      },
      "outputs": [],
      "source": [
        "\n",
        "PATH = '/content/convnext/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrChwbscbYkj",
        "outputId": "2209bb3c-84d8-4d3f-bf12-a496f2786c50"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/60: Train Acc 0.0780%, Train Loss 8.7148, Learning Rate 0.0999\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation:0:0.4571%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/60: Train Acc 1.3293%, Train Loss 8.0538, Learning Rate 0.0997\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/60: Train Acc 7.7045%, Train Loss 7.2784, Learning Rate 0.0994\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/60: Train Acc 22.4481%, Train Loss 6.4843, Learning Rate 0.0989\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/60: Train Acc 40.6751%, Train Loss 5.7904, Learning Rate 0.0983\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/60: Train Acc 56.0161%, Train Loss 5.2421, Learning Rate 0.0976\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation:5:56.0543%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/60: Train Acc 67.4121%, Train Loss 4.8255, Learning Rate 0.0967\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/60: Train Acc 75.3112%, Train Loss 4.5140, Learning Rate 0.0957\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/60: Train Acc 80.5968%, Train Loss 4.2847, Learning Rate 0.0946\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/60: Train Acc 84.5968%, Train Loss 4.1029, Learning Rate 0.0933\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/60: Train Acc 87.1831%, Train Loss 3.9651, Learning Rate 0.0919\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation:10:75.8171%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/60: Train Acc 89.2521%, Train Loss 3.8509, Learning Rate 0.0905\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/60: Train Acc 91.0586%, Train Loss 3.7540, Learning Rate 0.0889\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14/60: Train Acc 92.2004%, Train Loss 3.6750, Learning Rate 0.0872\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15/60: Train Acc 93.4209%, Train Loss 3.6027, Learning Rate 0.0854\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16/60: Train Acc 94.4404%, Train Loss 3.5413, Learning Rate 0.0835\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation:15:79.7800%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17/60: Train Acc 95.3075%, Train Loss 3.4883, Learning Rate 0.0815\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18/60: Train Acc 96.2025%, Train Loss 3.4376, Learning Rate 0.0794\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19/60: Train Acc 97.0424%, Train Loss 3.3892, Learning Rate 0.0772\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20/60: Train Acc 97.7478%, Train Loss 3.3487, Learning Rate 0.0750\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21/60: Train Acc 98.2186%, Train Loss 3.3126, Learning Rate 0.0727\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation:20:76.5686%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22/60: Train Acc 98.5777%, Train Loss 3.2855, Learning Rate 0.0703\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23/60: Train Acc 98.8460%, Train Loss 3.2567, Learning Rate 0.0679\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24/60: Train Acc 99.0163%, Train Loss 3.2371, Learning Rate 0.0655\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25/60: Train Acc 99.2094%, Train Loss 3.2165, Learning Rate 0.0629\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26/60: Train Acc 99.3633%, Train Loss 3.1964, Learning Rate 0.0604\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation:25:75.6314%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27/60: Train Acc 99.4391%, Train Loss 3.1799, Learning Rate 0.0578\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28/60: Train Acc 99.5192%, Train Loss 3.1654, Learning Rate 0.0552\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29/60: Train Acc 99.5936%, Train Loss 3.1504, Learning Rate 0.0526\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30/60: Train Acc 99.6401%, Train Loss 3.1384, Learning Rate 0.0500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 31/60: Train Acc 99.6716%, Train Loss 3.1273, Learning Rate 0.0474\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation:30:81.3257%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 32/60: Train Acc 99.7475%, Train Loss 3.1145, Learning Rate 0.0448\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 33/60: Train Acc 99.7689%, Train Loss 3.1041, Learning Rate 0.0422\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 34/60: Train Acc 99.8190%, Train Loss 3.0939, Learning Rate 0.0396\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 35/60: Train Acc 99.8340%, Train Loss 3.0850, Learning Rate 0.0371\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 36/60: Train Acc 99.8397%, Train Loss 3.0744, Learning Rate 0.0345\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation:35:80.2857%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 37/60: Train Acc 99.8848%, Train Loss 3.0648, Learning Rate 0.0321\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 38/60: Train Acc 99.8712%, Train Loss 3.0583, Learning Rate 0.0297\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 39/60: Train Acc 99.9134%, Train Loss 3.0494, Learning Rate 0.0273\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 40/60: Train Acc 99.9056%, Train Loss 3.0430, Learning Rate 0.0250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 41/60: Train Acc 99.9113%, Train Loss 3.0367, Learning Rate 0.0228\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation:40:84.2600%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 42/60: Train Acc 99.9306%, Train Loss 3.0291, Learning Rate 0.0206\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 43/60: Train Acc 99.9242%, Train Loss 3.0242, Learning Rate 0.0185\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 44/60: Train Acc 99.9471%, Train Loss 3.0183, Learning Rate 0.0165\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 45/60: Train Acc 99.9478%, Train Loss 3.0131, Learning Rate 0.0146\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 46/60: Train Acc 99.9463%, Train Loss 3.0083, Learning Rate 0.0128\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation:45:86.4914%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 47/60: Train Acc 99.9535%, Train Loss 3.0036, Learning Rate 0.0111\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 48/60: Train Acc 99.9521%, Train Loss 2.9994, Learning Rate 0.0095\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 49/60: Train Acc 99.9614%, Train Loss 2.9953, Learning Rate 0.0081\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50/60: Train Acc 99.9707%, Train Loss 2.9922, Learning Rate 0.0067\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 51/60: Train Acc 99.9671%, Train Loss 2.9895, Learning Rate 0.0054\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation:50:87.8086%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 52/60: Train Acc 99.9671%, Train Loss 2.9873, Learning Rate 0.0043\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 53/60: Train Acc 99.9700%, Train Loss 2.9847, Learning Rate 0.0033\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 54/60: Train Acc 99.9814%, Train Loss 2.9823, Learning Rate 0.0024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 55/60: Train Acc 99.9714%, Train Loss 2.9812, Learning Rate 0.0017\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 56/60: Train Acc 99.9664%, Train Loss 2.9802, Learning Rate 0.0011\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation:55:88.8057%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 57/60: Train Acc 99.9685%, Train Loss 2.9791, Learning Rate 0.0006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 58/60: Train Acc 99.9649%, Train Loss 2.9788, Learning Rate 0.0003\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 59/60: Train Acc 99.9757%, Train Loss 2.9779, Learning Rate 0.0001\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 60/60: Train Acc 99.9750%, Train Loss 2.9773, Learning Rate 0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    # Quality of life tip: leave=False and position=0 are needed to make tqdm usable in jupyter\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "\n",
        "    num_correct = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, (x, y) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        # if torch.cuda.is_available():\n",
        "        x = x.cuda()\n",
        "        y = y.cuda()\n",
        "\n",
        "        # Don't be surprised - we just wrap these two lines to make it work for FP16\n",
        "        with torch.cuda.amp.autocast():     \n",
        "            outputs = model(x)\n",
        "            loss = criterion(outputs, y)\n",
        "\n",
        "        # Update # correct & loss as we go\n",
        "        num_correct += int((torch.argmax(outputs, axis=1) == y).sum())\n",
        "        total_loss += float(loss)\n",
        "\n",
        "        # tqdm lets you add some details so you can monitor training as you train.\n",
        "        batch_bar.set_postfix(\n",
        "            acc=\"{:.04f}%\".format(100 * num_correct / ((i + 1) * batch_size)),\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            num_correct=num_correct,\n",
        "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "        \n",
        "        # Another couple things you need for FP16. \n",
        "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        scaler.update() # This is something added just for FP16\n",
        "\n",
        "        scheduler.step() # We told scheduler T_max that we'd call step() (len(train_loader) * epochs) many times.\n",
        "\n",
        "        batch_bar.update() # Update tqdm bar\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "\n",
        "    print(\"Epoch {}/{}: Train Acc {:.04f}%, Train Loss {:.04f}, Learning Rate {:.04f}\".format(\n",
        "        epoch + 1,\n",
        "        epochs,\n",
        "        100 * num_correct / (len(train_loader) * batch_size),\n",
        "        float(total_loss / len(train_loader)),\n",
        "        float(optimizer.param_groups[0]['lr'])))\n",
        "    \n",
        "\n",
        "    # You can add validation per-epoch here if you would like\n",
        "    if epoch%5 ==0:\n",
        "      model.eval()\n",
        "      batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "      num_correct = 0\n",
        "      for i, (x, y) in enumerate(val_loader):\n",
        "        x = x.cuda()\n",
        "        y = y.cuda()\n",
        "        with torch.no_grad():\n",
        "          outputs = model(x)\n",
        "\n",
        "        num_correct += int((torch.argmax(outputs, axis=1) == y).sum())\n",
        "        batch_bar.set_postfix(acc=\"{:.04f}%\".format(100 * num_correct / ((i + 1) * batch_size)))\n",
        "\n",
        "        batch_bar.update()\n",
        "      batch_bar.close()\n",
        "      print(\"Validation:\"+str(epoch)+\":\"+\"{:.04f}%\".format(100 * num_correct / len(val_dataset)))\n",
        "\n",
        "\n",
        "      torch.save(model.state_dict(), PATH+\"epoch\"+str(epoch)+\".pt\")\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5QZego7iqAzX"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), PATH+\"epoch\"+str(epoch)+\".pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKb2iD_9gdpX"
      },
      "source": [
        "# Classification Task: Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LVl9y10ew4oZ"
      },
      "outputs": [],
      "source": [
        "# !zip -r /outputs.zip /content/convent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xH4ZQbWdyy5L"
      },
      "outputs": [],
      "source": [
        "# !unzip -q /content/outputs.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "le1o-OVjfeN9",
        "outputId": "20463a61-1dda-43e0-b85a-5cb5ea8e800b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final_Validation: 88.9629%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "\n",
        "model.eval()\n",
        "batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "num_correct = 0\n",
        "for i, (x, y) in enumerate(val_loader):\n",
        "\n",
        "    x = x.cuda()\n",
        "    y = y.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(x)\n",
        "\n",
        "    num_correct += int((torch.argmax(outputs, axis=1) == y).sum())\n",
        "    batch_bar.set_postfix(acc=\"{:.04f}%\".format(100 * num_correct / ((i + 1) * batch_size)))\n",
        "\n",
        "    batch_bar.update()\n",
        "    \n",
        "batch_bar.close()\n",
        "print(\"Final_Validation: {:.04f}%\".format(100 * num_correct / len(val_dataset)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UpgCHImRkYQW"
      },
      "source": [
        "# Classification Task: Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "08Zv2AWFrfVP"
      },
      "outputs": [],
      "source": [
        "class ClassificationTestSet(Dataset):\n",
        "    # It's possible to load test set data using ImageFolder without making a custom class.\n",
        "    # See if you can think it through!\n",
        "\n",
        "    def __init__(self, data_dir, transforms):\n",
        "        self.data_dir = data_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # This one-liner basically generates a sorted list of full paths to each image in data_dir\n",
        "        self.img_paths = list(map(lambda fname: osp.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.transforms(Image.open(self.img_paths[idx]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "td_qvGwr16z0",
        "outputId": "96bde988-eead-49e8-8c43-cc27f8166933"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<__main__.ClassificationTestSet object at 0x7f0c6047b590>\n"
          ]
        }
      ],
      "source": [
        "test_dataset = ClassificationTestSet(TEST_DIR, ttf.Compose(val_transforms))\n",
        "print(test_dataset)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
        "                         drop_last=False, num_workers=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2WQEUjXkWvo",
        "outputId": "b8fb7758-ae58-418e-e0d1-1cc49fb4157c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "res = []\n",
        "model.eval()\n",
        "batch_bar = tqdm(total=len(test_loader), dynamic_ncols=True, position=0, leave=False, desc='Test')\n",
        "\n",
        "\n",
        "for i, (x) in enumerate(test_loader):\n",
        "\n",
        "    x = x.cuda()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(x)\n",
        "        pred_y = torch.argmax(outputs, axis=1)\n",
        "        res.extend(pred_y.tolist())\n",
        "    \n",
        "\n",
        "    batch_bar.update()\n",
        "\n",
        "batch_bar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vob9a2-HkW_V",
        "outputId": "dba918e6-9a67-4147-e1b5-16d9f5e6054b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "35000\n",
            "35000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "with open(\"/content/classification_early_submission.csv\", \"w+\") as f:\n",
        "    f.write(\"id,label\\n\")\n",
        "    print(len(test_dataset))\n",
        "    print(len(res))\n",
        "    for i in range(len(test_dataset)):\n",
        "        f.write(\"{},{}\\n\".format(str(i).zfill(6) + \".jpg\", res[i]))\n",
        "        \n",
        "torch.save(model.state_dict(), \"/content/Final_model_final.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsJx1l1T4twC"
      },
      "source": [
        "# Verification Task: Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoBFFF8-Lpvj"
      },
      "source": [
        "There are 6K verification dev images, but 166K \"pairs\" for you to compare. So, it's much more efficient to compute the features for the 6K verification images, and just compare afterwards.\n",
        "\n",
        "This will be done by creating a dictionary mapping the image file names to the features. Then, you'll use this dictionary to compute the similarities for each pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "m1YtIwxuL7H0"
      },
      "outputs": [],
      "source": [
        "class VerificationDataset(Dataset):\n",
        "    def __init__(self, data_dir, transforms):\n",
        "        self.data_dir = data_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # This one-liner basically generates a sorted list of full paths to each image in data_dir\n",
        "        self.img_paths = list(map(lambda fname: osp.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # We return the image, as well as the path to that image (relative path)\n",
        "        return self.transforms(Image.open(self.img_paths[idx])), osp.relpath(self.img_paths[idx], self.data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "98lmjm0S4tHR"
      },
      "outputs": [],
      "source": [
        "\n",
        "val_veri_dataset = VerificationDataset(osp.join(DATA_DIR, \"verification/verification/dev\"),\n",
        "                                       ttf.Compose(val_transforms))\n",
        "val_ver_loader = torch.utils.data.DataLoader(val_veri_dataset, batch_size=batch_size, \n",
        "                                             shuffle=False, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Qw45H-eMyyn",
        "outputId": "61e333b7-1745-4396-dce8-96f367907bd8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "feats_dict = dict()\n",
        "for batch_idx, (imgs, path_names) in tqdm(enumerate(val_ver_loader), total=len(val_ver_loader), position=0, leave=False):\n",
        "    imgs = imgs.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        feats = model(imgs) \n",
        "        feats = nn.GELU()(feats)\n",
        "        for i in range(len(feats)):\n",
        "            address='dev/'+path_names[i]\n",
        "            feats_dict[address] = feats[i]          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6TG6RD6NTtX",
        "outputId": "3faa4b09-e7c3-477d-9a3f-503b6fbca832"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.5595, device='cuda:0')"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# # What does this dict look like?\n",
        "z=feats_dict['dev/ab001b21a1.jpg']\n",
        "x=feats_dict['dev/10246770ce.jpg']\n",
        "nn.CosineSimilarity(dim=0)(z,x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zuqds2qNO6N",
        "outputId": "b31471cf-a4bf-4f97-9208-85ff76d99e10"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor(0.5595, device='cuda:0') tensor(0.1142, device='cuda:0')\n",
            " tensor(0.2032, device='cuda:0') ... tensor(0.2885, device='cuda:0')\n",
            " tensor(0.4706, device='cuda:0') tensor(0.7287, device='cuda:0')]\n",
            "[1 0 0 ... 0 1 1]\n",
            "AUC: 0.9649446129731225\n"
          ]
        }
      ],
      "source": [
        "# We use cosine similarity between feature embeddings.\n",
        "similarity_metric = nn.CosineSimilarity(dim=0)\n",
        "\n",
        "val_veri_csv = osp.join(DATA_DIR, \"verification/verification/verification_dev.csv\")\n",
        "\n",
        "\n",
        "# Now, loop through the csv and compare each pair, getting the similarity between them\n",
        "pred_similarities = []\n",
        "gt_similarities = []\n",
        "for line in tqdm(open(val_veri_csv).read().splitlines()[1:], position=0, leave=False): # skip header\n",
        "    img_path1, img_path2, gt = line.split(\",\")\n",
        "\n",
        "\n",
        "    # TODO: Use the similarity metric\n",
        "    # How to use these img_paths? What to do with the features?\n",
        "    similarity = similarity_metric(feats_dict[img_path1],feats_dict[img_path2])\n",
        "\n",
        "    pred_similarities.append(similarity)\n",
        "    gt_similarities.append(int(gt))\n",
        "\n",
        "pred_similarities = np.array(pred_similarities)\n",
        "print(pred_similarities)\n",
        "gt_similarities = np.array(gt_similarities)\n",
        "print(gt_similarities)\n",
        "\n",
        "print(\"AUC:\", roc_auc_score(gt_similarities, pred_similarities))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sakRa8oZOlKr"
      },
      "source": [
        "# Verification Task: Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "oDK3knDcOrOE"
      },
      "outputs": [],
      "source": [
        "test_veri_dataset = VerificationDataset(osp.join(DATA_DIR, \"verification/verification/test\"),\n",
        "                                        ttf.Compose(val_transforms))\n",
        "test_ver_loader = torch.utils.data.DataLoader(test_veri_dataset, batch_size=batch_size, \n",
        "                                              shuffle=False, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igeRT3WxOrB_",
        "outputId": "3b5fadee-8d3a-4141-9573-495a2dd946d1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "feats_final_dict = dict()\n",
        "for batch_idx, (imgs, path_names) in tqdm(enumerate(test_ver_loader), total=len(test_ver_loader), position=0, leave=False):\n",
        "    imgs = imgs.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        feats = model(imgs)\n",
        "        feats = nn.GELU()(feats) \n",
        "        for i in range(len(feats)):\n",
        "            address='test/'+path_names[i]\n",
        "            feats_final_dict[address] = feats[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4OZL_FNOq1r",
        "outputId": "69ae56dd-a576-436f-fe67-963bb68e64d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "\n",
        "val_veri_csv = osp.join(DATA_DIR, \"verification/verification/verification_test.csv\")\n",
        "\n",
        "\n",
        "pred_similarities = []\n",
        "for line in tqdm(open(val_veri_csv).read().splitlines()[1:], position=0, leave=False): # skip header\n",
        "    img_path1, img_path2 = line.split(\",\")\n",
        "\n",
        "    similarity = similarity_metric(feats_final_dict[img_path1],feats_final_dict[img_path2])\n",
        "   \n",
        "    pred_similarities.append(float(similarity))\n",
        "    \n",
        "\n",
        "pred_similarities = np.array(pred_similarities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcN20SqMNEu5",
        "outputId": "f2403111-e950-4006-c01b-3e47e2d0e09e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.43448931 0.60703105 0.53404409 ... 0.18287095 0.46324837 0.75750363]\n"
          ]
        }
      ],
      "source": [
        "print(pred_similarities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "fYXiglWkPBDv"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/verification_early_submission.csv\", \"w+\") as f:\n",
        "    f.write(\"id,match\\n\")\n",
        "    for i in range(len(pred_similarities)):\n",
        "        f.write(\"{},{}\\n\".format(i, pred_similarities[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALiq9PTl7KwY"
      },
      "source": [
        "# Extras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "OuAsK_tKhzH9"
      },
      "outputs": [],
      "source": [
        "# # If you keep re-initializing your model in Colab, can run out of GPU memory, need to restart.\n",
        "# # These three lines can help that - run this before you re-initialize your model\n",
        "\n",
        "# del model\n",
        "# torch.cuda.empty_cache()\n",
        "# !nvidia-smi"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Final_hw2p2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
